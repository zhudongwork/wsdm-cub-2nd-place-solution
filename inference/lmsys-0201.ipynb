{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5c0778",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-05T07:35:10.241976Z",
     "iopub.status.busy": "2024-08-05T07:35:10.240978Z",
     "iopub.status.idle": "2024-08-05T07:35:10.247863Z",
     "shell.execute_reply": "2024-08-05T07:35:10.245857Z",
     "shell.execute_reply.started": "2024-08-05T07:35:10.241927Z"
    },
    "papermill": {
     "duration": 0.004227,
     "end_time": "2025-02-03T17:36:13.039951",
     "exception": false,
     "start_time": "2025-02-03T17:36:13.035724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "604663f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T17:36:13.049255Z",
     "iopub.status.busy": "2025-02-03T17:36:13.048486Z",
     "iopub.status.idle": "2025-02-03T17:36:59.741154Z",
     "shell.execute_reply": "2025-02-03T17:36:59.740129Z"
    },
    "papermill": {
     "duration": 46.702507,
     "end_time": "2025-02-03T17:36:59.746167",
     "exception": false,
     "start_time": "2025-02-03T17:36:13.043660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton==2.2.0) (3.13.1)\r\n",
      "Installing collected packages: triton\r\n",
      "Successfully installed triton-2.2.0\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091a9a63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T17:36:59.754865Z",
     "iopub.status.busy": "2025-02-03T17:36:59.754584Z",
     "iopub.status.idle": "2025-02-03T17:37:48.581514Z",
     "shell.execute_reply": "2025-02-03T17:37:48.580403Z"
    },
    "papermill": {
     "duration": 48.837078,
     "end_time": "2025-02-03T17:37:48.587049",
     "exception": false,
     "start_time": "2025-02-03T17:36:59.749971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl\r\n",
      "Requirement already satisfied: torch>=1.12 in /opt/conda/lib/python3.10/site-packages (from xformers==0.0.24042abc8.d20240802) (2.1.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xformers==0.0.24042abc8.d20240802) (1.26.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (2024.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.12->xformers==0.0.24042abc8.d20240802) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.12->xformers==0.0.24042abc8.d20240802) (1.3.0)\r\n",
      "Installing collected packages: xformers\r\n",
      "Successfully installed xformers-0.0.24+042abc8.d20240802\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1102c926",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T17:37:48.596361Z",
     "iopub.status.busy": "2025-02-03T17:37:48.596065Z",
     "iopub.status.idle": "2025-02-03T17:37:49.711187Z",
     "shell.execute_reply": "2025-02-03T17:37:49.710065Z"
    },
    "papermill": {
     "duration": 1.122293,
     "end_time": "2025-02-03T17:37:49.713412",
     "exception": false,
     "start_time": "2025-02-03T17:37:48.591119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/wsdm-modules-0202/wsdm-modules-0202 human_pref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515e04a",
   "metadata": {
    "papermill": {
     "duration": 0.004161,
     "end_time": "2025-02-03T17:37:49.722242",
     "exception": false,
     "start_time": "2025-02-03T17:37:49.718081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19066771",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T17:37:49.732371Z",
     "iopub.status.busy": "2025-02-03T17:37:49.731766Z",
     "iopub.status.idle": "2025-02-03T17:37:49.738320Z",
     "shell.execute_reply": "2025-02-03T17:37:49.737361Z"
    },
    "papermill": {
     "duration": 0.013791,
     "end_time": "2025-02-03T17:37:49.740190",
     "exception": false,
     "start_time": "2025-02-03T17:37:49.726399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prepare_test_file.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prepare_test_file.py\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_parquet(\"/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet\")\n",
    "df[\"winner_model_a\"] = 1\n",
    "df[\"winner_model_b\"] = 0\n",
    "df = df.fillna(\"none\")\n",
    "# df['prompt'] = df['prompt'].apply(lambda x:x[:6000])\n",
    "df.to_parquet(\"test.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d106605",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T17:37:49.749208Z",
     "iopub.status.busy": "2025-02-03T17:37:49.748969Z",
     "iopub.status.idle": "2025-02-03T17:37:52.152868Z",
     "shell.execute_reply": "2025-02-03T17:37:52.151917Z"
    },
    "papermill": {
     "duration": 2.411218,
     "end_time": "2025-02-03T17:37:52.155466",
     "exception": false,
     "start_time": "2025-02-03T17:37:49.744248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python prepare_test_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179f876",
   "metadata": {
    "papermill": {
     "duration": 0.003945,
     "end_time": "2025-02-03T17:37:52.163821",
     "exception": false,
     "start_time": "2025-02-03T17:37:52.159876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference: gemma2-9b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74b3ee8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T17:37:52.173573Z",
     "iopub.status.busy": "2025-02-03T17:37:52.173316Z",
     "iopub.status.idle": "2025-02-03T17:37:52.180749Z",
     "shell.execute_reply": "2025-02-03T17:37:52.179935Z"
    },
    "papermill": {
     "duration": 0.014654,
     "end_time": "2025-02-03T17:37:52.182523",
     "exception": false,
     "start_time": "2025-02-03T17:37:52.167869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing predict_m0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predict_m0.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# from xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\n",
    "from human_pref.models.modeling_gemma2 import Gemma2ForSequenceClassification\n",
    "from human_pref.data.processors import ProcessorPAB, MIDProcessorPAB, MIDV2ProcessorPAB\n",
    "from human_pref.data.dataset import LMSYSDataset\n",
    "from human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\n",
    "from human_pref.utils import to_device\n",
    "\n",
    "import argparse\n",
    "\n",
    "# 创建 ArgumentParser 对象\n",
    "parser = argparse.ArgumentParser(description='这是一个示例程序。')\n",
    "parser.add_argument('--csv_path', default='test.parquet', help='csv path')\n",
    "parser.add_argument('--save_path', default='prob_m0.npy', help='csv path')\n",
    "args = parser.parse_args()\n",
    "\n",
    "model_name_or_path = \"/kaggle/input/m0-lmsys-mid2-final-step\"\n",
    "csv_path = args.csv_path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "processor = MIDV2ProcessorPAB(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=4096,\n",
    "    support_system_role=False,\n",
    ")\n",
    "dataset = LMSYSDataset(\n",
    "    csv_file=csv_path,\n",
    "    query=None,\n",
    "    processor=processor,\n",
    "    include_swap=False,\n",
    "    is_parquet=True,\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=80,\n",
    "    num_workers=4,\n",
    "    collate_fn=ShardedMaxTokensCollator(\n",
    "        max_tokens=8192, base_collator=VarlenCollator()\n",
    "    ),\n",
    ")\n",
    "\n",
    "# model for pipelined inference\n",
    "num_hidden_layers = 42\n",
    "device_map = {\n",
    "    \"model.embed_tokens\": \"cuda:0\",\n",
    "    \"model.norm\": \"cuda:1\",\n",
    "    \"score\": \"cuda:1\",\n",
    "}\n",
    "for i in range(num_hidden_layers // 2):\n",
    "    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\n",
    "for i in range(num_hidden_layers // 2, num_hidden_layers):\n",
    "    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n",
    "\n",
    "model = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# inv_freq clones for each device\n",
    "config = model.config\n",
    "dim = config.head_dim\n",
    "inv_freq = 1.0 / (\n",
    "    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n",
    ")\n",
    "inv_freq0 = inv_freq.to(\"cuda:0\")\n",
    "inv_freq1 = inv_freq.to(\"cuda:1\")\n",
    "\n",
    "\n",
    "# for name, p in model.named_parameters():\n",
    "#     print(name, p.device)\n",
    "# for name, b in model.model.named_buffers():\n",
    "#     print(name, b.device)\n",
    "\n",
    "# pipeline parallelism with two GPUs\n",
    "is_first = True\n",
    "hidden_states = None\n",
    "outs = []\n",
    "for batch in tqdm(dataloader):\n",
    "    for micro_batch in batch:\n",
    "        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n",
    "        seq_info = dict(\n",
    "            cu_seqlens=micro_batch[\"cu_seqlens\"],\n",
    "            position_ids=micro_batch[\"position_ids\"],\n",
    "            max_seq_len=micro_batch[\"max_seq_len\"],\n",
    "            # attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n",
    "        )\n",
    "        seq_info = to_device(seq_info, \"cuda:0\")\n",
    "        if is_first:\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n",
    "            is_first = False\n",
    "            prev_seq_info, prev_hidden_states = to_device(\n",
    "                [seq_info, prev_hidden_states], \"cuda:1\"\n",
    "            )\n",
    "            continue\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n",
    "            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n",
    "\n",
    "            prev_seq_info, prev_hidden_states = to_device(\n",
    "                [seq_info, hidden_states], \"cuda:1\"\n",
    "            )\n",
    "            outs.append(logits.cpu())\n",
    "\n",
    "# last micro-batch\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n",
    "    outs.append(logits.cpu())\n",
    "\n",
    "pred = torch.cat(outs, dim=0)\n",
    "prob = pred.softmax(-1)\n",
    "print(dataset.evaluate(prob.numpy()))\n",
    "\n",
    "np.save(args.save_path, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4243dff3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T17:37:52.191700Z",
     "iopub.status.busy": "2025-02-03T17:37:52.191469Z",
     "iopub.status.idle": "2025-02-03T17:37:52.194706Z",
     "shell.execute_reply": "2025-02-03T17:37:52.193959Z"
    },
    "papermill": {
     "duration": 0.009713,
     "end_time": "2025-02-03T17:37:52.196451",
     "exception": false,
     "start_time": "2025-02-03T17:37:52.186738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python predict_m0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c607120b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T17:37:52.206241Z",
     "iopub.status.busy": "2025-02-03T17:37:52.206027Z",
     "iopub.status.idle": "2025-02-03T17:41:31.803499Z",
     "shell.execute_reply": "2025-02-03T17:41:31.802401Z"
    },
    "papermill": {
     "duration": 219.605959,
     "end_time": "2025-02-03T17:41:31.806455",
     "exception": false,
     "start_time": "2025-02-03T17:37:52.200496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [03:07<00:00, 46.76s/it]\r\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]2025-02-03 17:41:11.480618: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2025-02-03 17:41:11.480805: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-02-03 17:41:11.658712: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:18<00:00, 18.05s/it]\r\n",
      "{'log_loss': 0.9207442536005698}\r\n"
     ]
    }
   ],
   "source": [
    "!python predict_m0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f1c843e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T17:41:31.821232Z",
     "iopub.status.busy": "2025-02-03T17:41:31.820598Z",
     "iopub.status.idle": "2025-02-03T17:41:31.826138Z",
     "shell.execute_reply": "2025-02-03T17:41:31.825324Z"
    },
    "papermill": {
     "duration": 0.013464,
     "end_time": "2025-02-03T17:41:31.827999",
     "exception": false,
     "start_time": "2025-02-03T17:41:31.814535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing make_submission_step1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile make_submission_step1.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "preds = np.load(\"prob_m0.npy\")\n",
    "df = pd.read_parquet(\"test.parquet\")\n",
    "df['winer_a'] = preds[:,0]\n",
    "df['winer_b'] = preds[:,1]\n",
    "df['abs'] = abs(preds[:,0] - preds[:,1])\n",
    "df.to_parquet(\"test.parquet\", index=False)\n",
    "\n",
    "df_swap = df[df['abs'] < 0.8]\n",
    "df_swap[\"response_a\"], df_swap[\"response_b\"] = df_swap[\"response_b\"], df_swap[\"response_a\"]\n",
    "df_swap.to_parquet(\"test_swap.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99f1e13e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T17:41:31.838469Z",
     "iopub.status.busy": "2025-02-03T17:41:31.838241Z",
     "iopub.status.idle": "2025-02-03T17:41:33.589710Z",
     "shell.execute_reply": "2025-02-03T17:41:33.588464Z"
    },
    "papermill": {
     "duration": 1.759133,
     "end_time": "2025-02-03T17:41:33.591884",
     "exception": false,
     "start_time": "2025-02-03T17:41:31.832751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python make_submission_step1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e7c49b",
   "metadata": {
    "papermill": {
     "duration": 0.004666,
     "end_time": "2025-02-03T17:41:33.601711",
     "exception": false,
     "start_time": "2025-02-03T17:41:33.597045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference: llama3-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64557dd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T17:41:33.612948Z",
     "iopub.status.busy": "2025-02-03T17:41:33.612673Z",
     "iopub.status.idle": "2025-02-03T17:41:33.620103Z",
     "shell.execute_reply": "2025-02-03T17:41:33.619244Z"
    },
    "papermill": {
     "duration": 0.015249,
     "end_time": "2025-02-03T17:41:33.621766",
     "exception": false,
     "start_time": "2025-02-03T17:41:33.606517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing predict_m3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predict_m3.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\n",
    "from human_pref.models.modeling_llama import LlamaForSequenceClassification\n",
    "from human_pref.data.processors import ProcessorPAB, MIDProcessorPAB, MIDV2ProcessorPAB\n",
    "from human_pref.data.dataset import LMSYSDataset\n",
    "from human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\n",
    "from human_pref.utils import to_device\n",
    "\n",
    "\n",
    "model_name_or_path = \"/kaggle/input/m3-lmsys-mid2-final-step\"\n",
    "csv_path = \"test_swap.parquet\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.deprecation_warnings[\n",
    "    \"sequence-length-is-longer-than-the-specified-maximum\"\n",
    "] = True\n",
    "processor = MIDV2ProcessorPAB(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=4096,\n",
    "    support_system_role=True,\n",
    ")\n",
    "dataset = LMSYSDataset(\n",
    "    csv_file=csv_path,\n",
    "    query=None,\n",
    "    processor=processor,\n",
    "    include_swap=False,\n",
    "    is_parquet=True,\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=80,\n",
    "    num_workers=4,\n",
    "    collate_fn=ShardedMaxTokensCollator(\n",
    "        max_tokens=8192, base_collator=VarlenCollator()\n",
    "    ),\n",
    ")\n",
    "\n",
    "# model for pipelined inference\n",
    "num_hidden_layers = 32\n",
    "device_map = {\n",
    "    \"model.embed_tokens\": \"cuda:0\",\n",
    "    \"model.norm\": \"cuda:1\",\n",
    "    \"score\": \"cuda:1\",\n",
    "}\n",
    "for i in range(num_hidden_layers // 2):\n",
    "    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\n",
    "for i in range(num_hidden_layers // 2, num_hidden_layers):\n",
    "    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n",
    "\n",
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# inv_freq clones for each device\n",
    "config = model.config\n",
    "dim = config.hidden_size // config.num_attention_heads\n",
    "inv_freq = 1.0 / (\n",
    "    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n",
    ")\n",
    "inv_freq0 = inv_freq.to(\"cuda:0\")\n",
    "inv_freq1 = inv_freq.to(\"cuda:1\")\n",
    "\n",
    "\n",
    "# for name, p in model.named_parameters():\n",
    "#     print(name, p.device)\n",
    "# for name, b in model.model.named_buffers():\n",
    "#     print(name, b.device)\n",
    "\n",
    "# pipeline parallelism with two GPUs\n",
    "is_first = True\n",
    "hidden_states = None\n",
    "outs = []\n",
    "for batch in tqdm(dataloader):\n",
    "    for micro_batch in batch:\n",
    "        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n",
    "        seq_info = dict(\n",
    "            cu_seqlens=micro_batch[\"cu_seqlens\"],\n",
    "            position_ids=micro_batch[\"position_ids\"],\n",
    "            max_seq_len=micro_batch[\"max_seq_len\"],\n",
    "            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n",
    "        )\n",
    "        seq_info = to_device(seq_info, \"cuda:0\")\n",
    "        if is_first:\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n",
    "            is_first = False\n",
    "            prev_seq_info, prev_hidden_states = to_device(\n",
    "                [seq_info, prev_hidden_states], \"cuda:1\"\n",
    "            )\n",
    "            continue\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n",
    "            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n",
    "\n",
    "            prev_seq_info, prev_hidden_states = to_device(\n",
    "                [seq_info, hidden_states], \"cuda:1\"\n",
    "            )\n",
    "            outs.append(logits.cpu())\n",
    "\n",
    "# last micro-batch\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n",
    "    outs.append(logits.cpu())\n",
    "\n",
    "\n",
    "pred = torch.cat(outs, dim=0)\n",
    "prob = pred.softmax(-1)\n",
    "print(dataset.evaluate(prob.numpy()))\n",
    "\n",
    "np.save('prob_m3.npy', prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ec0bb27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T17:41:33.632409Z",
     "iopub.status.busy": "2025-02-03T17:41:33.632189Z",
     "iopub.status.idle": "2025-02-03T17:44:17.881778Z",
     "shell.execute_reply": "2025-02-03T17:44:17.880183Z"
    },
    "papermill": {
     "duration": 164.258292,
     "end_time": "2025-02-03T17:44:17.884954",
     "exception": false,
     "start_time": "2025-02-03T17:41:33.626662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [02:29<00:00, 37.26s/it]\r\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]2025-02-03 17:44:08.316117: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2025-02-03 17:44:08.316197: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-02-03 17:44:08.318065: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.20s/it]\r\n",
      "{'log_loss': 0.5988653580475317}\r\n"
     ]
    }
   ],
   "source": [
    "!python predict_m3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bc8fe5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T17:44:17.908075Z",
     "iopub.status.busy": "2025-02-03T17:44:17.907304Z",
     "iopub.status.idle": "2025-02-03T17:44:17.915570Z",
     "shell.execute_reply": "2025-02-03T17:44:17.914630Z"
    },
    "papermill": {
     "duration": 0.020615,
     "end_time": "2025-02-03T17:44:17.918116",
     "exception": false,
     "start_time": "2025-02-03T17:44:17.897501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing make_submission_step2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile make_submission_step2.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# preds = np.load(\"prob_m3.npy\")\n",
    "df_swap = pd.read_parquet(\"test_swap.parquet\")\n",
    "preds = np.average(\n",
    "    [\n",
    "        df_swap[['winer_a','winer_b']].values,\n",
    "        np.load(\"prob_m3.npy\")[:, [1, 0]],\n",
    "    ],\n",
    "    axis=0,\n",
    "    weights=[2.5, 1],\n",
    ")\n",
    "\n",
    "df = pd.read_parquet(\"test.parquet\")\n",
    "\n",
    "df['winner'] = (df['winer_a'] >= df['winer_b'])\n",
    "df.loc[df['abs'] < 0.8, 'winner'] = (preds[:,0] > preds[:,1])\n",
    "df['winner'] = df['winner'].map({True: 'model_a', False: 'model_b'})\n",
    "\n",
    "sub = df[['id', 'winner']]\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81b058be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T17:44:17.939556Z",
     "iopub.status.busy": "2025-02-03T17:44:17.938926Z",
     "iopub.status.idle": "2025-02-03T17:44:19.964685Z",
     "shell.execute_reply": "2025-02-03T17:44:19.963737Z"
    },
    "papermill": {
     "duration": 2.038,
     "end_time": "2025-02-03T17:44:19.966758",
     "exception": false,
     "start_time": "2025-02-03T17:44:17.928758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id   winner\r\n",
      "0   327228  model_b\r\n",
      "1  1139415  model_a\r\n",
      "2  1235630  model_a\r\n"
     ]
    }
   ],
   "source": [
    "!python make_submission_step2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56de74d0",
   "metadata": {
    "papermill": {
     "duration": 0.005654,
     "end_time": "2025-02-03T17:44:19.978604",
     "exception": false,
     "start_time": "2025-02-03T17:44:19.972950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10131489,
     "sourceId": 86946,
     "sourceType": "competition"
    },
    {
     "datasetId": 5493674,
     "sourceId": 9102725,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5496847,
     "sourceId": 9107963,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6388480,
     "sourceId": 10318738,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6410480,
     "sourceId": 10352229,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6590309,
     "sourceId": 10643610,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6596316,
     "sourceId": 10652420,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6596829,
     "sourceId": 10653146,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 489.911931,
   "end_time": "2025-02-03T17:44:20.202240",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-03T17:36:10.290309",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
